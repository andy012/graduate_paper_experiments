package com.shu.nita.spark.videoabstract;

/**
 * Created by havstack on 3/30/15.
 */


import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.InputSplit;
import org.apache.hadoop.mapreduce.JobContext;
import org.apache.hadoop.mapreduce.RecordReader;
import org.apache.hadoop.mapreduce.TaskAttemptContext;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;

import java.io.IOException;

/**
 * <p>实现使用hadoop分析视频文件，该类将视频文件解析成帧数和帧图像的键值对</p>
 *
 */
public class VideoInputFormat extends FileInputFormat<Text, Text> {
    public static final Log LOG =
            LogFactory.getLog(VideoInputFormat.class);
    public VideoInputFormat(){}
    /**
     * Generate the list of files and make them into FileSplits.
     * @param job the job context
     * @throws IOException
     */

    /*
    @Override
    public List<InputSplit> getSplits(JobContext job) throws IOException {
        Stopwatch sw = new Stopwatch().start();
        long minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));
        long maxSize = getMaxSplitSize(job);
        // generate splits
        List<InputSplit> splits = new ArrayList<InputSplit>();
        List<FileStatus> files = listStatus(job);
        for (FileStatus file: files) {
            splits.addAll(getSplitsSingleFile(file,job));
//            if (length != 0) {
//                BlockLocation[] blkLocations;
//                if (file instanceof LocatedFileStatus) {
//                    blkLocations = ((LocatedFileStatus) file).getBlockLocations();
//                } else {
//                    FileSystem fs = path.getFileSystem(job.getConfiguration());
//                    blkLocations = fs.getFileBlockLocations(file, 0, length);
//                }
//                if (isSplitable(job, path)) {
//                    long blockSize = file.getBlockSize();
//                    long splitSize = computeSplitSize(blockSize, minSize, maxSize);
//
//                    long bytesRemaining = length;
//                    while (((double) bytesRemaining)/splitSize > SPLIT_SLOP) {
//                        int blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);
//                        splits.add(makeSplit(path, length-bytesRemaining, splitSize,
//                                blkLocations[blkIndex].getHosts(),
//                                blkLocations[blkIndex].getCachedHosts()));
//                        bytesRemaining -= splitSize;
//                    }
//
//                    if (bytesRemaining != 0) {
//                        int blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);
//                        splits.add(makeSplit(path, length-bytesRemaining, bytesRemaining,
//                                blkLocations[blkIndex].getHosts(),
//                                blkLocations[blkIndex].getCachedHosts()));
//                    }
//                } else { // not splitable
//                    splits.add(makeSplit(path, 0, length, blkLocations[0].getHosts(),
//                            blkLocations[0].getCachedHosts()));
//                }
//            } else {
//                //Create empty hosts array for zero length files
//                splits.add(makeSplit(path, 0, length, new String[0]));
//            }
        }
        // Save the number of input files for metrics/loadgen
        sw.stop();
        if (LOG.isDebugEnabled()) {
            LOG.debug("Total # of splits generated by getSplits: " + splits.size()
                    + ", TimeTaken: " + sw.elapsedMillis());
        }
        return splits;
    }
    protected org.apache.hadoop.mapreduce.lib.input.FileSplit makeSplit(Path file, long start, long length, String[] hosts) {
        return new org.apache.hadoop.mapreduce.lib.input.FileSplit(file, start, length, hosts);
    }

    protected org.apache.hadoop.mapreduce.lib.input.FileSplit makeSplit(Path file, long start, long length, String[] hosts, String[] inMemoryHosts) {
        return new org.apache.hadoop.mapreduce.lib.input.FileSplit(file, start, length, hosts, inMemoryHosts);
    }
    public List<InputSplit> getSplitsSingleFile(FileStatus file,JobContext job) throws IOException {
        // generate splits
        List<InputSplit> splits = new ArrayList<InputSplit>();
        Path path=file.getPath();
        FileSystem fs = path.getFileSystem(job.getConfiguration());
        BlockLocation[] blockLocations;
        if(file instanceof LocatedFileStatus) {
            blockLocations = ((LocatedFileStatus)file).getBlockLocations();
        } else {
            blockLocations = fs.getFileBlockLocations(file, 0L, file.getLen());
        }
        FSDataInputStream fsIn=fs.open(path);
        ArrayList<Long> gop_offset_list=new ArrayList<Long>();
        for(BlockLocation bl:blockLocations){
            //fsIn.seek();
            Long gop_offset=getGopOffset(fsIn,bl.getOffset());
            gop_offset_list.add(gop_offset);
        }
        fsIn.close();
        //System.out.println(gop_offset_list.size());

        for(int i=0;i<blockLocations.length-1;++i){
            System.out.println(gop_offset_list.get(i));
            if(gop_offset_list.get(i+1)!=-1){
                //if(gop_offset_list.get(i+1)==blockLocations[i+1].getOffset()) {
                splits.add(makeSplit(path,
                        gop_offset_list.get(i),
                        gop_offset_list.get(i + 1) - gop_offset_list.get(i),
                        blockLocations[i].getHosts()
                ));
            }else if(gop_offset_list.get(i+1)!=-1 && i+1==blockLocations.length-1) {
                splits.add(
                        makeSplit(path, gop_offset_list.get(i),
                                file.getLen() - gop_offset_list.get(i),
                                blockLocations[i].getHosts()
                        ));
            }


        }
        if(gop_offset_list.get(gop_offset_list.size()-1)>0){
            splits.add(makeSplit(path, gop_offset_list.get(gop_offset_list.size()-1),
                    file.getLen() - gop_offset_list.get(gop_offset_list.size()-1),
                    blockLocations[gop_offset_list.size()-1].getHosts(),
                    blockLocations[gop_offset_list.size()-1].getCachedHosts()));
        }
        return splits;
    }*/

    /**
     * 获取当前数据块的第一个gop的起始位置
     */
   /* public Long getGopOffset(FSDataInputStream fis,long block_start){

        long offset=-1;
        int read_size=1024*1024*2;
        byte[] byte_array=new byte[read_size];
        try {
            fis.seek(block_start);

            for(int i=0;i<read_size;i+=65536){
                System.out.println(fis.getPos() + "read-size=>" + fis.read(byte_array, i, 65536));
            }

            // System.out.println(byte_array[65535]);
            ByteBuffer byteBuffer=ByteBuffer.wrap(byte_array);


            ByteBuffer segment;
            while ((segment = H264Utils.nextNALUnit(byteBuffer)) != null) {

                if(NALUnit.read(segment).type== NALUnitType.SPS){
                    offset=segment.arrayOffset()+block_start-4;
                    System.out.println(block_start+"offset="+offset);
                    break;
                }
            }
        } catch (IOException e) {
            e.printStackTrace();
        }

        return offset;
    }*/















    /* (non-Javadoc)
    * <p>Title: createRecordReader</p>
    * <p>Description: </p>
    * @param arg0
    * @param arg1
    * @return
    * @throws IOException
    * @throws InterruptedException
    * @see org.apache.hadoop.mapreduce.InputFormat#createRecordReader(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)
    */
    @Override
    public RecordReader<Text, Text> createRecordReader(
            InputSplit split, TaskAttemptContext context) throws IOException,
            InterruptedException {

        return new VideoRecordReader();
    }

    //确保一个视频一个mapper处理
    protected boolean isSplitable(JobContext context, Path file) {
        return false;
    }

}
